{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd42b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 12:26:40.884577: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-13 12:26:42.103959: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-13 12:26:42.106025: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-03-13 12:26:42.141513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 44.49GiB deviceMemoryBandwidth: 581.23GiB/s\n",
      "2022-03-13 12:26:42.141776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:5e:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 44.49GiB deviceMemoryBandwidth: 581.23GiB/s\n",
      "2022-03-13 12:26:42.141794: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-13 12:26:42.148360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x7f90465052c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-13 12:26:42.148405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-03-13 12:26:42.179064: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-13 12:26:42.180188: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-13 12:26:42.182107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-13 12:26:42.185112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-13 12:26:42.186053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-13 12:26:42.186889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2022-03-13 12:26:42.187438: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-13 12:26:42.189780: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-13 12:26:42.341757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 44.49GiB deviceMemoryBandwidth: 581.23GiB/s\n",
      "2022-03-13 12:26:42.342012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:5e:00.0 name: Quadro RTX 8000 computeCapability: 7.5\n",
      "coreClock: 1.62GHz coreCount: 72 deviceMemorySize: 44.49GiB deviceMemoryBandwidth: 581.23GiB/s\n",
      "2022-03-13 12:26:42.342044: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-13 12:26:42.342073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-13 12:26:42.342084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-03-13 12:26:42.342095: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-13 12:26:42.342105: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-13 12:26:42.342116: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-13 12:26:42.342126: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-13 12:26:42.342137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-13 12:26:42.342851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2022-03-13 12:26:42.342883: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-13 12:26:43.129043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-03-13 12:26:43.129078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2022-03-13 12:26:43.129083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y \n",
      "2022-03-13 12:26:43.129086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N \n",
      "2022-03-13 12:26:43.130035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1883 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:3b:00.0, compute capability: 7.5)\n",
      "2022-03-13 12:26:43.130836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 42242 MB memory) -> physical GPU (device: 1, name: Quadro RTX 8000, pci bus id: 0000:5e:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import nn\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Lambda, Dropout\n",
    "from tensorflow.keras import Input, Model\n",
    "from utils import *\n",
    "py_file_location = \"...\"\n",
    "os.path.abspath(os.path.join(os.path.dirname(py_file_location), os.path.pardir))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.device('GPU:1') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a091b4f7",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14dd0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbr_train = int(1E6)\n",
    "\n",
    "Nbr_test = int(2E5) \n",
    "\n",
    "#outfile = 'Dataset/' # choose your path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ca7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbr_test_bruteforce = 10000\n",
    "\n",
    "\n",
    "project_sub_path = \"Imperfect CSI Data/CF_DNN\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"\"\n",
    "### Train ###\n",
    "#dataset_train_Anne\n",
    "dataset_train = np.load(os.path.join(parent_dir,project_sub_path,'dataset_train_GF.npz'))\n",
    "\n",
    "h_11_tr = dataset_train['h_PP'][:Nbr_train]\n",
    "h_12_tr = dataset_train['h_PS'][:Nbr_train]\n",
    "h_1R_tr = dataset_train['h_PR'][:Nbr_train]\n",
    "h_21_tr = dataset_train['h_SP'][:Nbr_train]\n",
    "h_22_tr = dataset_train['h_SS'][:Nbr_train]\n",
    "h_2R_tr = dataset_train['h_SR'][:Nbr_train]\n",
    "h_R1_tr = dataset_train['h_RP'][:Nbr_train]\n",
    "h_R2_tr = dataset_train['h_RS'][:Nbr_train]\n",
    "\n",
    "\n",
    "#x_train_input = np.stack([h_11_tr, h_12_tr, h_1R_tr, h_21_tr, h_22_tr, h_2R_tr, h_R1_tr, h_R2_tr], axis=1)\n",
    "x_train = np.stack([h_R1_tr, h_11_tr, h_2R_tr, h_1R_tr, h_22_tr, h_R2_tr, h_21_tr, h_12_tr], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### VAL ###\n",
    "  \n",
    "# Parent Directory path\n",
    "#dataset_val_Anne\n",
    "\n",
    "dataset_test = np.load(os.path.join(parent_dir,project_sub_path,'dataset_val_GF.npz'))\n",
    "\n",
    "h_11_test = dataset_test['h_PP'][:Nbr_test_bruteforce]\n",
    "h_12_test = dataset_test['h_PS'][:Nbr_test_bruteforce]\n",
    "h_1R_test = dataset_test['h_PR'][:Nbr_test_bruteforce]\n",
    "h_21_test = dataset_test['h_SP'][:Nbr_test_bruteforce]\n",
    "h_22_test = dataset_test['h_SS'][:Nbr_test_bruteforce]\n",
    "h_2R_test = dataset_test['h_SR'][:Nbr_test_bruteforce]\n",
    "h_R1_test = dataset_test['h_RP'][:Nbr_test_bruteforce]\n",
    "h_R2_test = dataset_test['h_RS'][:Nbr_test_bruteforce]\n",
    "\n",
    "x_test = np.stack([h_R1_test, h_11_test, h_2R_test, h_1R_test, h_22_test, h_R2_test, h_21_test, h_12_test], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798a5fb",
   "metadata": {},
   "source": [
    "# CF loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a81506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_CF(Lambda=10**0.5, v_tau=0.25):\n",
    "    \n",
    "    def CF_loss(G, y_out):\n",
    "        \n",
    "        ''' compute loss for CF Relaying'''\n",
    "        \n",
    "        Tau = tf.constant(v_tau, dtype=tf.float32) # ==> Tau \n",
    "\n",
    "        W = tf.constant(Lambda, dtype=tf.float32)  # ==> lambda \n",
    "\n",
    "        G = tf.cast(G, dtype='float32')\n",
    "        \n",
    "        y_out = tf.cast(y_out, dtype='float32')\n",
    "\n",
    "        # index retrieval\n",
    "\n",
    "        Grp_indx, Gpp_indx, Gsr_indx, Gpr_indx, Gss_indx, Grs_indx, Gsp_indx, Gps_indx  = [0], [1], [2], [3], [4], [5], [6], [7]\n",
    "        \n",
    "        Pr_indx, Ps_indx  = [0], [1]\n",
    "\n",
    "        # tensors retrieval\n",
    "        \n",
    "        Grp, Gpp, Gsr, Gpr, Gss, Grs, Gsp, Gps, Pr, Ps = tf.gather(G, Grp_indx, axis=1), tf.gather(G, Gpp_indx, axis=1), tf.gather(G, Gsr_indx, axis=1), tf.gather(G, Gpr_indx, axis=1), tf.gather(G, Gss_indx, axis=1), tf.gather(G, Grs_indx, axis=1), tf.gather(G, Gsp_indx, axis=1), tf.gather(G, Gps_indx, axis=1), tf.gather(y_out, Pr_indx, axis=1), tf.gather(y_out, Ps_indx, axis=1)\n",
    "\n",
    "        #  Primary power Creation\n",
    "\n",
    "        Pp = tf.multiply(tf.ones(tf.shape(Pr), dtype=tf.dtypes.float32),10.0)\n",
    "\n",
    "        # NS_Tilde :NS = gPS*PP +1\n",
    "\n",
    "        NS_Tilde = tf.add(tf.multiply(Gps,Pp),tf.constant(1, dtype=tf.float32))\n",
    "\n",
    "        #NR_Tilde : gPR*PP +1\n",
    "\n",
    "        NR_Tilde = tf.add(tf.multiply(Gpr,Pp),tf.constant(1, dtype=tf.float32))\n",
    "\n",
    "        # Rho_Z : sqrt(Gpr*Gps)*Pp/sqrt(NR_tilde*NS_tilde)\n",
    "\n",
    "        Rho_Z = tf.divide(tf.multiply(tf.sqrt(tf.multiply(Gpr,Gps)),Pp),tf.sqrt(tf.multiply(NR_Tilde,NS_Tilde)))\n",
    "\n",
    "        #K1 : Gsr*NS_Tilde+Gss*NR_Tilde-2*Rho_Z*sqrt(Gsr*Gss*NR_Tilde*NS_Tilde)\n",
    "\n",
    "        E1 = tf.add(tf.multiply(Gsr,NS_Tilde),tf.multiply(Gss,NR_Tilde))\n",
    "\n",
    "        E2 = tf.sqrt(tf.multiply(tf.multiply(tf.multiply(Gsr,Gss),NR_Tilde),NS_Tilde))\n",
    "\n",
    "        K1 = tf.math.subtract(E1,tf.multiply(tf.multiply(tf.constant(2, dtype=tf.float32),Rho_Z),E2))\n",
    "\n",
    "        #K2 : (1-Rho_Z**2)*NR_Tilde*NS_Tilde\n",
    "        \n",
    "        K2 = tf.multiply(tf.math.subtract(tf.constant(1, dtype=tf.float32),tf.pow(Rho_Z,tf.constant(2, dtype=tf.float32))),tf.multiply(NR_Tilde,NS_Tilde))\n",
    "\n",
    "        # function A' ==> A'(Gpp) : ((Gpp*Pp)/((1+(Gpp*Pp))**(1-tau)-1))-1 ==> (Gpp*Pp)/(R1) \n",
    "\n",
    "        R1 = tf.add(tf.constant(1, dtype=tf.float32),tf.multiply(Gpp,Pp))\n",
    "        R1 = tf.pow(R1, tf.math.subtract(tf.constant(1, dtype=tf.float32),Tau))\n",
    "        R1 = tf.math.subtract(R1,tf.constant(1, dtype=tf.float32))\n",
    "\n",
    "        A_ = tf.subtract(tf.divide(tf.multiply(Gpp,Pp),R1),tf.constant(1, dtype=tf.float32))\n",
    "\n",
    "        # QoS : \n",
    "\n",
    "        # PR**2 and PS**2 because custom_sigmoid(x):  \n",
    "        # returns : Output of sigmoid function range between 0 and sqrt(10)\n",
    "\n",
    "        Qos = tf.add(tf.multiply(Gsp,tf.pow(Ps,2)),tf.multiply(Grp,tf.pow(Pr,2)))\n",
    "\n",
    "        Qos = tf.subtract(Qos, A_)\n",
    "\n",
    "        Qos = tf.multiply(W,tf.keras.activations.relu(Qos)) \n",
    "\n",
    "        # SNR : \n",
    "        num = tf.add(tf.multiply(tf.multiply(tf.multiply(K1,Grs),\\\n",
    "                                             tf.pow(Ps,2)),tf.pow(Pr,2)),\\\n",
    "                     tf.add(tf.multiply(tf.multiply(Gss,tf.pow(Ps,2)),tf.multiply(K1,tf.pow(Ps,2))),\\\n",
    "                     tf.multiply(tf.multiply(Gss,tf.pow(Ps,2)),K2)))\n",
    "        \n",
    "        d_num = tf.add(tf.multiply(tf.multiply(K2,Grs), tf.pow(Pr,2)),\\\n",
    "                       tf.add(tf.multiply(NS_Tilde,tf.multiply(K1,tf.pow(Ps,2))),\\\n",
    "                       tf.multiply(NS_Tilde, K2)))\n",
    "        \n",
    "        SNR_opt = tf.divide(num, d_num)\n",
    "\n",
    "        # R_S\n",
    "\n",
    "        Rs_opt =  tf.multiply(tf.constant(0.5, dtype=tf.float32),log2(tf.add(tf.constant(1,dtype=tf.float32),SNR_opt)))\n",
    "\n",
    "        #-n_SNR+n_Qos\n",
    "        res = tf.reduce_mean(-Rs_opt+Qos) \n",
    "\n",
    "\n",
    "        return res\n",
    "    return CF_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ba6cc",
   "metadata": {},
   "source": [
    "# DNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62151762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_CF(X_train, f_loss, f_metrics, f_activation1, f_activation2, LR) :\n",
    "    \"\"\"\n",
    "\n",
    "      Structure of DL model for DF.\n",
    "\n",
    "      Parameters:\n",
    "         X_train: Channel gain array.\n",
    "         f_loss: Loss function.\n",
    "         f_metrics: List of metrics.\n",
    "         f_activation1: First activation function for the output.\n",
    "         f_activation2: Second activation function for the output.\n",
    "         LR : Learning rate.\n",
    "         \n",
    "      Returns:\n",
    "        DL model for CF.\n",
    "    \"\"\"\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
    "    inputs = Input(shape=(X_train.shape[1]))\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    output1 = Dense(1, activation=f_activation1)(x)\n",
    "    output2 = Dense(1, activation=f_activation2)(x)\n",
    "    merged = tf.keras.layers.Concatenate()([output1, output2])\n",
    "    model = Model(inputs=inputs, outputs=[merged])\n",
    "    model.compile(loss=f_loss, optimizer=opt, metrics=f_metrics)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a324ac97",
   "metadata": {},
   "source": [
    "# Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd92403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------ tensorflow functions for DNN ------------# \n",
    "def custom_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Modified sigmoid function used for handling predicted powers.\n",
    "\n",
    "    Parameters:\n",
    "      x: tensor.\n",
    "    Returns:\n",
    "      Output of sigmoid function range between 0 and sqrt(10)\n",
    "    \"\"\"\n",
    "    output = tf.multiply(tf.sqrt(tf.constant(10,dtype=tf.float32)),nn.sigmoid(x))\n",
    "    # Cache the logits to use for crossentropy loss.\n",
    "    output._keras_logits = x  # pylint: disable=protected-access\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log2(x):\n",
    "    numerator = tf.math.log(x)\n",
    "    denominator = tf.math.log(tf.constant(2, dtype=tf.float32))\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def Delta_DNN(G, y_out):\n",
    " \n",
    "    \"\"\"\n",
    "      Metrics used on DL model for Primary achievable rate degradation.\n",
    "      \n",
    "      Parameters:\n",
    "        G: Channel gain tensor.\n",
    "        y_out: Predicted parameter.\n",
    "      Returns:\n",
    "        Primary achievable rate degradation for each samples \n",
    "    \"\"\"\n",
    "    G = tf.cast(G, dtype='float32')\n",
    "    y_out = tf.cast(y_out, dtype='float32')\n",
    "    \n",
    "    # index retrieval\n",
    "\n",
    "\n",
    "    Grp_indx, Gpp_indx, Gsp_indx = [0], [1], [6]\n",
    "    Pr_indx, Ps_indx  = [0], [1]\n",
    "\n",
    "    # tensors retrieval\n",
    "    Grp, Gpp, Gsp, Pr, Ps = tf.gather(G, Grp_indx, axis=1), tf.gather(G, Gpp_indx, axis=1), tf.gather(G, Gsp_indx, axis=1),  tf.gather(y_out, Pr_indx, axis=1), tf.gather(y_out, Ps_indx, axis=1)\n",
    "    \n",
    "    \n",
    "    #  Pp Creation\n",
    "    Pp = tf.multiply(tf.ones(tf.shape(Pr), dtype=tf.dtypes.float32),10)\n",
    "    \n",
    "    # Rp : C((Gpp*Pp)/(Grp*Pr**2+Gsp*Ps**2+2*(np.sqrt(Gsp*Grp)*Alpha*Ps*Pr)+1) ==> C(H1/H2), where H1 = Gpp*Pp and H2 = R1+R2+1\n",
    "    H1 = tf.multiply(Gpp, Pp)\n",
    "\n",
    "    R1 =  tf.add(tf.multiply(Grp, tf.pow(Pr, 2)),tf.multiply(Gsp,tf.pow(Ps, 2)))\n",
    "\n",
    "    H2 = tf.add(R1, tf.constant(1,dtype=tf.float32))\n",
    "    \n",
    "    SNR_P = tf.divide(H1, H2)\n",
    "    \n",
    "    Rp =  tf.multiply(tf.constant(0.5, dtype=tf.float32),log2(tf.add(tf.constant(1,dtype=tf.float32),SNR_P)))\n",
    "\n",
    "    # Rp_ : C(Gpp*Pp)\n",
    "    SNR_P_ = tf.multiply(Gpp, Pp)\n",
    "    Rp_ = tf.multiply(tf.constant(0.5,dtype=tf.float32),log2(tf.add(tf.constant(1,dtype=tf.float32),SNR_P_)))\n",
    "    # 1 - ratio(Rp, Rp_)\n",
    "    RRP = tf.subtract(tf.constant(1,dtype=tf.float32), tf.divide(Rp, Rp_))\n",
    "    \n",
    "    return RRP\n",
    "\n",
    "\n",
    "\n",
    "def Opportunistic_Achievable_Rate(v_tau):\n",
    "    def opportunistic_rate(G, y_out):\n",
    "        \"\"\"\n",
    "        Metrics used on DL model for throughput calculation.\n",
    "        This function will get those parameters as input\n",
    "        G: Channel gain tensor.\n",
    "        y_out: Predicted parameter.\n",
    "\n",
    "        Parameters:\n",
    "        Lambda : Penalty for QoS\n",
    "        Tau : degradation factor for the primary network\n",
    "        Returns:\n",
    "        Throughput mean \n",
    "        \"\"\"\n",
    "\n",
    "        Tau = tf.constant(v_tau, dtype=tf.float32) # ==> Tau \n",
    "\n",
    "        G = tf.cast(G, dtype='float32')\n",
    "        y_out = tf.cast(y_out, dtype='float32')\n",
    "\n",
    "        # index retrieval\n",
    "    \n",
    "\n",
    "\n",
    "        Grp_indx, Gpp_indx, Gsr_indx, Gpr_indx, Gss_indx, Grs_indx, Gsp_indx, Gps_indx = [0], [1], [2], [3], [4], [5], [6], [7]\n",
    "        Pr_indx, Ps_indx  = [0], [1]\n",
    "\n",
    "        # tensors retrieval\n",
    "        Grp, Gpp, Gsr, Gpr, Gss, Grs, Gsp, Gps, Pr, Ps = tf.gather(G, Grp_indx, axis=1), tf.gather(G, Gpp_indx, axis=1), tf.gather(G, Gsr_indx, axis=1), tf.gather(G, Gpr_indx, axis=1), tf.gather(G, Gss_indx, axis=1), tf.gather(G, Grs_indx, axis=1), tf.gather(G, Gsp_indx, axis=1), tf.gather(G, Gps_indx, axis=1), tf.gather(y_out, Pr_indx, axis=1), tf.gather(y_out, Ps_indx, axis=1)\n",
    "        Pp = tf.multiply(tf.ones(tf.shape(Pr), dtype=tf.dtypes.float32),10)\n",
    "\n",
    "\n",
    "        NS_Tilde = tf.add(tf.multiply(Gps,Pp),tf.constant(1, dtype=tf.float32))\n",
    "\n",
    "        #NR_Tilde : gPR*PP +1\n",
    "\n",
    "        NR_Tilde = tf.add(tf.multiply(Gpr,Pp),tf.constant(1, dtype=tf.float32))\n",
    "\n",
    "        # Rho_Z : sqrt(Gpr*Gps)*Pp/sqrt(NR_tilde*NS_tilde)\n",
    "\n",
    "        Rho_Z = tf.divide(tf.multiply(tf.sqrt(tf.multiply(Gpr,Gps)),Pp),tf.sqrt(tf.multiply(NR_Tilde,NS_Tilde)))\n",
    "\n",
    "\n",
    "        #K1 : Gsr*NS_Tilde+Gss*NR_Tilde-2*Rho_Z*sqrt(Gsr*Gss*NR_Tilde*NS_Tilde)\n",
    "\n",
    "        E1 = tf.add(tf.multiply(Gsr,NS_Tilde),tf.multiply(Gss,NR_Tilde))\n",
    "\n",
    "        E2 = tf.sqrt(tf.multiply(tf.multiply(tf.multiply(Gsr,Gss),NR_Tilde),NS_Tilde))\n",
    "\n",
    "        K1 = tf.math.subtract(E1,tf.multiply(tf.multiply(tf.constant(2, dtype=tf.float32),Rho_Z),E2))\n",
    "\n",
    "        #K2 : (1-Rho_Z**2)*NR_Tilde*NS_Tilde\n",
    "\n",
    "        K2 = tf.multiply(tf.math.subtract(tf.constant(1, dtype=tf.float32),tf.pow(Rho_Z,tf.constant(2, dtype=tf.float32))),tf.multiply(NR_Tilde,NS_Tilde))\n",
    "\n",
    "        num = tf.add(tf.multiply(tf.multiply(tf.multiply(K1,Grs),tf.pow(Ps,2)),tf.pow(Pr,2)),tf.add(tf.multiply(tf.multiply(Gss,tf.pow(Ps,2)),tf.multiply(K1,tf.pow(Ps,2))),tf.multiply(tf.multiply(Gss,tf.pow(Ps,2)),K2)))\n",
    "                \n",
    "        d_num = tf.add(tf.multiply(tf.multiply(K2,Grs), tf.pow(Pr,2)),tf.add(tf.multiply(NS_Tilde,tf.multiply(K1,tf.pow(Ps,2))),tf.multiply(NS_Tilde,K2)))\n",
    "        \n",
    "        SNR_opt = tf.divide(num, d_num)\n",
    "\n",
    "        #C = 1/2*log2(1+SNR_opt)\n",
    "        # debit calculation\n",
    "        Rs = tf.multiply(tf.constant(0.5,dtype=tf.float32),log2(tf.add(tf.constant(1,dtype=tf.float32),SNR_opt)))\n",
    "        return Rs\n",
    "    return opportunistic_rate\n",
    "\n",
    "\n",
    "def outage_percentage(v_tau): \n",
    "    def outage_percentage(G, y_out): \n",
    "        \"\"\"\n",
    "          metrics used on DL model for testing Primary achievable rate degradation percentage .\n",
    "\n",
    "          Parameters:\n",
    "            G: Channel gain tensor.\n",
    "            y_out: Predicted parameter.\n",
    "          Returns:\n",
    "            percentage of Primary achievable rate degradation \n",
    "        \"\"\"\n",
    "        Tau = tf.constant(v_tau, dtype=tf.float32) # ==> Tau \n",
    "\n",
    "        G = tf.cast(G, dtype='float32')\n",
    "\n",
    "        y_out = tf.cast(y_out, dtype='float32')\n",
    "\n",
    "        # index retrieval\n",
    "\n",
    "\n",
    "        Grp_indx, Gpp_indx, Gsp_indx = [0], [1], [6]\n",
    "        Pr_indx, Ps_indx  = [0], [1]\n",
    "\n",
    "        # tensors retrieval\n",
    "        Grp, Gpp, Gsp, Pr, Ps = tf.gather(G, Grp_indx, axis=1), tf.gather(G, Gpp_indx, axis=1), tf.gather(G, Gsp_indx, axis=1),  tf.gather(y_out, Pr_indx, axis=1), tf.gather(y_out, Ps_indx, axis=1)\n",
    "\n",
    "\n",
    "        #  Pp Creation\n",
    "        Pp = tf.multiply(tf.ones(tf.shape(Pr), dtype=tf.dtypes.float32),10)\n",
    "\n",
    "        # Rp : C((Gpp*Pp)/(Grp*Pr**2+Gsp*Ps**2+2*(np.sqrt(Gsp*Grp)*Alpha*Ps*Pr)+1) ==> C(H1/H2), where H1 = Gpp*Pp and H2 = R1+R2+1\n",
    "        H1 = tf.multiply(Gpp, Pp)\n",
    "\n",
    "        R1 =  tf.add(tf.multiply(Grp, tf.pow(Pr, 2)),tf.multiply(Gsp,tf.pow(Ps, 2)))\n",
    "\n",
    "        H2 = tf.add(R1, tf.constant(1,dtype=tf.float32))\n",
    "\n",
    "        SNR_P = tf.divide(H1, H2)\n",
    "\n",
    "        Rp =  tf.multiply(tf.constant(0.5, dtype=tf.float32),log2(tf.add(tf.constant(1,dtype=tf.float32),SNR_P)))\n",
    "\n",
    "        # Rp_ : C(Gpp*Pp)\n",
    "        SNR_P_ = tf.multiply(Gpp, Pp)\n",
    "        Rp_ = tf.multiply(tf.constant(0.5,dtype=tf.float32),log2(tf.add(tf.constant(1,dtype=tf.float32),SNR_P_)))\n",
    "        # 1 - ratio(Rp, Rp_)\n",
    "        RRP = tf.subtract(tf.constant(1,dtype=tf.float32), tf.divide(Rp, Rp_))\n",
    "\n",
    "        # 1 - ratio(Rp, Rp_)\n",
    "        ARD = tf.subtract(tf.constant(1,dtype=tf.float32), tf.divide(Rp, Rp_))\n",
    "\n",
    "        #ARD > tau  \n",
    "        mask_PDD = tf.greater(ARD, Tau)# boolean array \n",
    "\n",
    "        return mask_PDD #*100 ?? \n",
    "\n",
    "    return outage_percentage\n",
    "\n",
    "\n",
    "def QoS_Violation(v_tau, QoS_thresh = -5): \n",
    "    def V_Qos(G, y_out): \n",
    "        \"\"\"\n",
    "        metrics used on DL model for testing QoS viloation .\n",
    "\n",
    "        Parameters:\n",
    "            G: Channel gain tensor.\n",
    "            y_out: Predicted parameter.\n",
    "        Returns:\n",
    "            Number of violated QoS \n",
    "        \"\"\"\n",
    "        Tau = tf.constant(v_tau, dtype=tf.float32) # ==> Tau \n",
    "\n",
    "        W = tf.constant(Lambda, dtype=tf.float32)  # ==> lambda \n",
    "\n",
    "        G = tf.cast(G, dtype='float32')\n",
    "\n",
    "        y_out = tf.cast(y_out, dtype='float32')\n",
    "\n",
    "        # index retrieval\n",
    "\n",
    "        Grp_indx, Gpp_indx, Gsr_indx, Gpr_indx, Gss_indx, Grs_indx, Gsp_indx, Gps_indx  = [0], [1], [2], [3], [4], [5], [6], [7]\n",
    "\n",
    "        Pr_indx, Ps_indx  = [0], [1]\n",
    "\n",
    "        # tensors retrieval\n",
    "\n",
    "        Grp, Gpp, Gsr, Gpr, Gss, Grs, Gsp, Gps, Pr, Ps = tf.gather(G, Grp_indx, axis=1), tf.gather(G, Gpp_indx, axis=1), tf.gather(G, Gsr_indx, axis=1), tf.gather(G, Gpr_indx, axis=1), tf.gather(G, Gss_indx, axis=1), tf.gather(G, Grs_indx, axis=1), tf.gather(G, Gsp_indx, axis=1), tf.gather(G, Gps_indx, axis=1), tf.gather(y_out, Pr_indx, axis=1), tf.gather(y_out, Ps_indx, axis=1)\n",
    "\n",
    "        #  Primary power Creation\n",
    "\n",
    "        Pp = tf.multiply(tf.ones(tf.shape(Pr), dtype=tf.dtypes.float32),10.0)\n",
    "\n",
    "        # function A' ==> A'(Gpp) : ((Gpp*Pp)/((1+(Gpp*Pp))**(1-tau)-1))-1 ==> (Gpp*Pp)/(R1) \n",
    "\n",
    "        R1 = tf.add(tf.constant(1, dtype=tf.float32),tf.multiply(Gpp,Pp))\n",
    "        R1 = tf.pow(R1, tf.math.subtract(tf.constant(1, dtype=tf.float32),Tau))\n",
    "        R1 = tf.math.subtract(R1,tf.constant(1, dtype=tf.float32))\n",
    "\n",
    "        A_ = tf.subtract(tf.divide(tf.multiply(Gpp,Pp),R1),tf.constant(1, dtype=tf.float32))\n",
    "\n",
    "        # QoS : \n",
    "\n",
    "        # PR**2 and PS**2 because custom_sigmoid(x):  \n",
    "        # returns : Output of sigmoid function range between 0 and sqrt(10)\n",
    "\n",
    "        Qos = tf.add(tf.multiply(Gsp,tf.pow(Ps,2)),tf.multiply(Grp,tf.pow(Pr,2)))\n",
    "\n",
    "        Qos = tf.subtract(Qos, A_)\n",
    "\n",
    "        Qos = tf.multiply(W,tf.keras.activations.relu(Qos)) \n",
    "\n",
    "        Qos = tf.subtract(Qos, A_)\n",
    "\n",
    "        #n_Qos = tf.divide(Qos, A_) # Normalization ?????\n",
    "        #Qos > 10**-5  \n",
    "        mask_pr = tf.greater(n_Qos,tf.math.pow(tf.constant(10, dtype=tf.float32),QoS_thresh))# boolean array \n",
    "\n",
    "        return mask_pr\n",
    "    return V_Qos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          1152        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          33024       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          65792       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            257         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            257         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2)            0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 166,274\n",
      "Trainable params: 166,274\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 12:27:19.342521: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-03-13 12:27:19.361759: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 12:27:21.384906: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27/196 [===>..........................] - ETA: 1s - loss: 36.7981 - Delta_DNN: 0.5151 - opportunistic_rate: 0.3876 - outage_percentage: 0.7521"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 12:27:22.034454: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 4s 9ms/step - loss: 12.1964 - Delta_DNN: 0.3240 - opportunistic_rate: 0.2022 - outage_percentage: 0.4922 - val_loss: -0.0042 - val_Delta_DNN: 0.0473 - val_opportunistic_rate: 0.0169 - val_outage_percentage: 0.0308\n",
      "Epoch 2/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.0073 - Delta_DNN: 0.0421 - opportunistic_rate: 0.0158 - outage_percentage: 0.0227 - val_loss: -0.0122 - val_Delta_DNN: 0.0359 - val_opportunistic_rate: 0.0176 - val_outage_percentage: 0.0155\n",
      "Epoch 3/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.0136 - Delta_DNN: 0.0363 - opportunistic_rate: 0.0192 - outage_percentage: 0.0160 - val_loss: -0.0200 - val_Delta_DNN: 0.0383 - val_opportunistic_rate: 0.0275 - val_outage_percentage: 0.0199\n",
      "Epoch 4/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.0292 - Delta_DNN: 0.0423 - opportunistic_rate: 0.0398 - outage_percentage: 0.0246 - val_loss: -0.0886 - val_Delta_DNN: 0.0603 - val_opportunistic_rate: 0.1054 - val_outage_percentage: 0.0412\n",
      "Epoch 5/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.1148 - Delta_DNN: 0.0650 - opportunistic_rate: 0.1364 - outage_percentage: 0.0404 - val_loss: -0.1628 - val_Delta_DNN: 0.0803 - val_opportunistic_rate: 0.1904 - val_outage_percentage: 0.0486\n",
      "Epoch 6/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.1731 - Delta_DNN: 0.0787 - opportunistic_rate: 0.1940 - outage_percentage: 0.0406 - val_loss: -0.2065 - val_Delta_DNN: 0.0822 - val_opportunistic_rate: 0.2243 - val_outage_percentage: 0.0405\n",
      "Epoch 7/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.2151 - Delta_DNN: 0.0855 - opportunistic_rate: 0.2357 - outage_percentage: 0.0456 - val_loss: -0.2431 - val_Delta_DNN: 0.0899 - val_opportunistic_rate: 0.2652 - val_outage_percentage: 0.0517\n",
      "Epoch 8/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.2478 - Delta_DNN: 0.0898 - opportunistic_rate: 0.2688 - outage_percentage: 0.0523 - val_loss: -0.2682 - val_Delta_DNN: 0.0928 - val_opportunistic_rate: 0.2908 - val_outage_percentage: 0.0606\n",
      "Epoch 9/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.2701 - Delta_DNN: 0.0905 - opportunistic_rate: 0.2907 - outage_percentage: 0.0556 - val_loss: -0.2837 - val_Delta_DNN: 0.0891 - val_opportunistic_rate: 0.3022 - val_outage_percentage: 0.0524\n",
      "Epoch 10/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.2841 - Delta_DNN: 0.0897 - opportunistic_rate: 0.3040 - outage_percentage: 0.0557 - val_loss: -0.2923 - val_Delta_DNN: 0.0873 - val_opportunistic_rate: 0.3092 - val_outage_percentage: 0.0509\n",
      "Epoch 11/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.2923 - Delta_DNN: 0.0889 - opportunistic_rate: 0.3118 - outage_percentage: 0.0561 - val_loss: -0.2981 - val_Delta_DNN: 0.0899 - val_opportunistic_rate: 0.3201 - val_outage_percentage: 0.0589\n",
      "Epoch 12/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.2982 - Delta_DNN: 0.0878 - opportunistic_rate: 0.3167 - outage_percentage: 0.0548 - val_loss: -0.3025 - val_Delta_DNN: 0.0874 - val_opportunistic_rate: 0.3205 - val_outage_percentage: 0.0533\n",
      "Epoch 13/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3011 - Delta_DNN: 0.0876 - opportunistic_rate: 0.3188 - outage_percentage: 0.0542 - val_loss: -0.3056 - val_Delta_DNN: 0.0876 - val_opportunistic_rate: 0.3247 - val_outage_percentage: 0.0541\n",
      "Epoch 14/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3057 - Delta_DNN: 0.0874 - opportunistic_rate: 0.3231 - outage_percentage: 0.0538 - val_loss: -0.3082 - val_Delta_DNN: 0.0859 - val_opportunistic_rate: 0.3247 - val_outage_percentage: 0.0487\n",
      "Epoch 15/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3077 - Delta_DNN: 0.0871 - opportunistic_rate: 0.3246 - outage_percentage: 0.0528 - val_loss: -0.3103 - val_Delta_DNN: 0.0868 - val_opportunistic_rate: 0.3261 - val_outage_percentage: 0.0507\n",
      "Epoch 16/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3099 - Delta_DNN: 0.0877 - opportunistic_rate: 0.3264 - outage_percentage: 0.0535 - val_loss: -0.3121 - val_Delta_DNN: 0.0861 - val_opportunistic_rate: 0.3257 - val_outage_percentage: 0.0473\n",
      "Epoch 17/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3117 - Delta_DNN: 0.0879 - opportunistic_rate: 0.3273 - outage_percentage: 0.0526 - val_loss: -0.3143 - val_Delta_DNN: 0.0867 - val_opportunistic_rate: 0.3275 - val_outage_percentage: 0.0459\n",
      "Epoch 18/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3144 - Delta_DNN: 0.0884 - opportunistic_rate: 0.3294 - outage_percentage: 0.0515 - val_loss: -0.3153 - val_Delta_DNN: 0.0928 - val_opportunistic_rate: 0.3372 - val_outage_percentage: 0.0661\n",
      "Epoch 19/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3164 - Delta_DNN: 0.0893 - opportunistic_rate: 0.3317 - outage_percentage: 0.0525 - val_loss: -0.3177 - val_Delta_DNN: 0.0894 - val_opportunistic_rate: 0.3321 - val_outage_percentage: 0.0497\n",
      "Epoch 20/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3160 - Delta_DNN: 0.0900 - opportunistic_rate: 0.3304 - outage_percentage: 0.0512 - val_loss: -0.3191 - val_Delta_DNN: 0.0878 - val_opportunistic_rate: 0.3295 - val_outage_percentage: 0.0416\n",
      "Epoch 21/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3183 - Delta_DNN: 0.0907 - opportunistic_rate: 0.3320 - outage_percentage: 0.0509 - val_loss: -0.3204 - val_Delta_DNN: 0.0945 - val_opportunistic_rate: 0.3395 - val_outage_percentage: 0.0624\n",
      "Epoch 22/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3195 - Delta_DNN: 0.0918 - opportunistic_rate: 0.3329 - outage_percentage: 0.0506 - val_loss: -0.3229 - val_Delta_DNN: 0.0914 - val_opportunistic_rate: 0.3341 - val_outage_percentage: 0.0458\n",
      "Epoch 23/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3222 - Delta_DNN: 0.0933 - opportunistic_rate: 0.3352 - outage_percentage: 0.0511 - val_loss: -0.3247 - val_Delta_DNN: 0.0950 - val_opportunistic_rate: 0.3392 - val_outage_percentage: 0.0554\n",
      "Epoch 24/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3234 - Delta_DNN: 0.0945 - opportunistic_rate: 0.3358 - outage_percentage: 0.0507 - val_loss: -0.3266 - val_Delta_DNN: 0.0951 - val_opportunistic_rate: 0.3387 - val_outage_percentage: 0.0484\n",
      "Epoch 25/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3255 - Delta_DNN: 0.0962 - opportunistic_rate: 0.3374 - outage_percentage: 0.0506 - val_loss: -0.3288 - val_Delta_DNN: 0.0968 - val_opportunistic_rate: 0.3404 - val_outage_percentage: 0.0479\n",
      "Epoch 26/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3286 - Delta_DNN: 0.0982 - opportunistic_rate: 0.3404 - outage_percentage: 0.0510 - val_loss: -0.3314 - val_Delta_DNN: 0.1010 - val_opportunistic_rate: 0.3448 - val_outage_percentage: 0.0552\n",
      "Epoch 27/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3307 - Delta_DNN: 0.1008 - opportunistic_rate: 0.3419 - outage_percentage: 0.0515 - val_loss: -0.3343 - val_Delta_DNN: 0.1021 - val_opportunistic_rate: 0.3444 - val_outage_percentage: 0.0488\n",
      "Epoch 28/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3325 - Delta_DNN: 0.1036 - opportunistic_rate: 0.3432 - outage_percentage: 0.0509 - val_loss: -0.3368 - val_Delta_DNN: 0.1025 - val_opportunistic_rate: 0.3430 - val_outage_percentage: 0.0360\n",
      "Epoch 29/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3362 - Delta_DNN: 0.1074 - opportunistic_rate: 0.3463 - outage_percentage: 0.0514 - val_loss: -0.3409 - val_Delta_DNN: 0.1088 - val_opportunistic_rate: 0.3491 - val_outage_percentage: 0.0450\n",
      "Epoch 30/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3412 - Delta_DNN: 0.1117 - opportunistic_rate: 0.3509 - outage_percentage: 0.0518 - val_loss: -0.3443 - val_Delta_DNN: 0.1185 - val_opportunistic_rate: 0.3585 - val_outage_percentage: 0.0681\n",
      "Epoch 31/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3444 - Delta_DNN: 0.1171 - opportunistic_rate: 0.3536 - outage_percentage: 0.0530 - val_loss: -0.3492 - val_Delta_DNN: 0.1199 - val_opportunistic_rate: 0.3575 - val_outage_percentage: 0.0490\n",
      "Epoch 32/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3498 - Delta_DNN: 0.1221 - opportunistic_rate: 0.3584 - outage_percentage: 0.0523 - val_loss: -0.3528 - val_Delta_DNN: 0.1245 - val_opportunistic_rate: 0.3596 - val_outage_percentage: 0.0469\n",
      "Epoch 33/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3519 - Delta_DNN: 0.1272 - opportunistic_rate: 0.3601 - outage_percentage: 0.0525 - val_loss: -0.3556 - val_Delta_DNN: 0.1289 - val_opportunistic_rate: 0.3631 - val_outage_percentage: 0.0460\n",
      "Epoch 34/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3557 - Delta_DNN: 0.1312 - opportunistic_rate: 0.3637 - outage_percentage: 0.0523 - val_loss: -0.3587 - val_Delta_DNN: 0.1352 - val_opportunistic_rate: 0.3678 - val_outage_percentage: 0.0574\n",
      "Epoch 35/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3571 - Delta_DNN: 0.1353 - opportunistic_rate: 0.3648 - outage_percentage: 0.0529 - val_loss: -0.3609 - val_Delta_DNN: 0.1387 - val_opportunistic_rate: 0.3688 - val_outage_percentage: 0.0567\n",
      "Epoch 36/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3603 - Delta_DNN: 0.1391 - opportunistic_rate: 0.3676 - outage_percentage: 0.0531 - val_loss: -0.3622 - val_Delta_DNN: 0.1427 - val_opportunistic_rate: 0.3712 - val_outage_percentage: 0.0632\n",
      "Epoch 37/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3615 - Delta_DNN: 0.1421 - opportunistic_rate: 0.3686 - outage_percentage: 0.0537 - val_loss: -0.3639 - val_Delta_DNN: 0.1414 - val_opportunistic_rate: 0.3692 - val_outage_percentage: 0.0422\n",
      "Epoch 38/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3634 - Delta_DNN: 0.1447 - opportunistic_rate: 0.3702 - outage_percentage: 0.0527 - val_loss: -0.3657 - val_Delta_DNN: 0.1491 - val_opportunistic_rate: 0.3750 - val_outage_percentage: 0.0685\n",
      "Epoch 39/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3648 - Delta_DNN: 0.1476 - opportunistic_rate: 0.3715 - outage_percentage: 0.0535 - val_loss: -0.3671 - val_Delta_DNN: 0.1500 - val_opportunistic_rate: 0.3746 - val_outage_percentage: 0.0602\n",
      "Epoch 40/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3665 - Delta_DNN: 0.1498 - opportunistic_rate: 0.3729 - outage_percentage: 0.0538 - val_loss: -0.3684 - val_Delta_DNN: 0.1520 - val_opportunistic_rate: 0.3762 - val_outage_percentage: 0.0633\n",
      "Epoch 41/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3682 - Delta_DNN: 0.1516 - opportunistic_rate: 0.3742 - outage_percentage: 0.0529 - val_loss: -0.3670 - val_Delta_DNN: 0.1583 - val_opportunistic_rate: 0.3788 - val_outage_percentage: 0.0936\n",
      "Epoch 42/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3691 - Delta_DNN: 0.1533 - opportunistic_rate: 0.3752 - outage_percentage: 0.0536 - val_loss: -0.3699 - val_Delta_DNN: 0.1551 - val_opportunistic_rate: 0.3769 - val_outage_percentage: 0.0596\n",
      "Epoch 43/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3672 - Delta_DNN: 0.1549 - opportunistic_rate: 0.3728 - outage_percentage: 0.0525 - val_loss: -0.3713 - val_Delta_DNN: 0.1562 - val_opportunistic_rate: 0.3770 - val_outage_percentage: 0.0520\n",
      "Epoch 44/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3692 - Delta_DNN: 0.1566 - opportunistic_rate: 0.3747 - outage_percentage: 0.0529 - val_loss: -0.3717 - val_Delta_DNN: 0.1582 - val_opportunistic_rate: 0.3786 - val_outage_percentage: 0.0620\n",
      "Epoch 45/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3704 - Delta_DNN: 0.1573 - opportunistic_rate: 0.3757 - outage_percentage: 0.0526 - val_loss: -0.3715 - val_Delta_DNN: 0.1610 - val_opportunistic_rate: 0.3805 - val_outage_percentage: 0.0738\n",
      "Epoch 46/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3712 - Delta_DNN: 0.1584 - opportunistic_rate: 0.3766 - outage_percentage: 0.0532 - val_loss: -0.3710 - val_Delta_DNN: 0.1636 - val_opportunistic_rate: 0.3810 - val_outage_percentage: 0.0829\n",
      "Epoch 47/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3691 - Delta_DNN: 0.1594 - opportunistic_rate: 0.3743 - outage_percentage: 0.0530 - val_loss: -0.3728 - val_Delta_DNN: 0.1568 - val_opportunistic_rate: 0.3756 - val_outage_percentage: 0.0305\n",
      "Epoch 48/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3710 - Delta_DNN: 0.1600 - opportunistic_rate: 0.3760 - outage_percentage: 0.0513 - val_loss: -0.3741 - val_Delta_DNN: 0.1605 - val_opportunistic_rate: 0.3784 - val_outage_percentage: 0.0459\n",
      "Epoch 49/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3716 - Delta_DNN: 0.1618 - opportunistic_rate: 0.3763 - outage_percentage: 0.0524 - val_loss: -0.3740 - val_Delta_DNN: 0.1590 - val_opportunistic_rate: 0.3772 - val_outage_percentage: 0.0361\n",
      "Epoch 50/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3728 - Delta_DNN: 0.1622 - opportunistic_rate: 0.3774 - outage_percentage: 0.0520 - val_loss: -0.3743 - val_Delta_DNN: 0.1643 - val_opportunistic_rate: 0.3799 - val_outage_percentage: 0.0595\n",
      "Epoch 51/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3738 - Delta_DNN: 0.1633 - opportunistic_rate: 0.3784 - outage_percentage: 0.0526 - val_loss: -0.3751 - val_Delta_DNN: 0.1617 - val_opportunistic_rate: 0.3785 - val_outage_percentage: 0.0383\n",
      "Epoch 52/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3742 - Delta_DNN: 0.1641 - opportunistic_rate: 0.3788 - outage_percentage: 0.0522 - val_loss: -0.3756 - val_Delta_DNN: 0.1640 - val_opportunistic_rate: 0.3804 - val_outage_percentage: 0.0548\n",
      "Epoch 53/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3734 - Delta_DNN: 0.1648 - opportunistic_rate: 0.3778 - outage_percentage: 0.0529 - val_loss: -0.3755 - val_Delta_DNN: 0.1628 - val_opportunistic_rate: 0.3789 - val_outage_percentage: 0.0404\n",
      "Epoch 54/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3730 - Delta_DNN: 0.1656 - opportunistic_rate: 0.3773 - outage_percentage: 0.0519 - val_loss: -0.3761 - val_Delta_DNN: 0.1655 - val_opportunistic_rate: 0.3807 - val_outage_percentage: 0.0546\n",
      "Epoch 55/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3744 - Delta_DNN: 0.1661 - opportunistic_rate: 0.3787 - outage_percentage: 0.0530 - val_loss: -0.3764 - val_Delta_DNN: 0.1666 - val_opportunistic_rate: 0.3813 - val_outage_percentage: 0.0572\n",
      "Epoch 56/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3736 - Delta_DNN: 0.1663 - opportunistic_rate: 0.3778 - outage_percentage: 0.0520 - val_loss: -0.3759 - val_Delta_DNN: 0.1636 - val_opportunistic_rate: 0.3788 - val_outage_percentage: 0.0383\n",
      "Epoch 57/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3730 - Delta_DNN: 0.1668 - opportunistic_rate: 0.3771 - outage_percentage: 0.0518 - val_loss: -0.3767 - val_Delta_DNN: 0.1702 - val_opportunistic_rate: 0.3828 - val_outage_percentage: 0.0714\n",
      "Epoch 58/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3760 - Delta_DNN: 0.1672 - opportunistic_rate: 0.3800 - outage_percentage: 0.0520 - val_loss: -0.3769 - val_Delta_DNN: 0.1680 - val_opportunistic_rate: 0.3814 - val_outage_percentage: 0.0561\n",
      "Epoch 59/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3766 - Delta_DNN: 0.1679 - opportunistic_rate: 0.3806 - outage_percentage: 0.0531 - val_loss: -0.3772 - val_Delta_DNN: 0.1678 - val_opportunistic_rate: 0.3808 - val_outage_percentage: 0.0456\n",
      "Epoch 60/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3756 - Delta_DNN: 0.1685 - opportunistic_rate: 0.3794 - outage_percentage: 0.0515 - val_loss: -0.3775 - val_Delta_DNN: 0.1694 - val_opportunistic_rate: 0.3822 - val_outage_percentage: 0.0596\n",
      "Epoch 61/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3772 - Delta_DNN: 0.1689 - opportunistic_rate: 0.3810 - outage_percentage: 0.0524 - val_loss: -0.3774 - val_Delta_DNN: 0.1705 - val_opportunistic_rate: 0.3829 - val_outage_percentage: 0.0673\n",
      "Epoch 62/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3758 - Delta_DNN: 0.1689 - opportunistic_rate: 0.3796 - outage_percentage: 0.0531 - val_loss: -0.3779 - val_Delta_DNN: 0.1690 - val_opportunistic_rate: 0.3817 - val_outage_percentage: 0.0475\n",
      "Epoch 63/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3755 - Delta_DNN: 0.1699 - opportunistic_rate: 0.3791 - outage_percentage: 0.0520 - val_loss: -0.3773 - val_Delta_DNN: 0.1651 - val_opportunistic_rate: 0.3795 - val_outage_percentage: 0.0298\n",
      "Epoch 64/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3770 - Delta_DNN: 0.1693 - opportunistic_rate: 0.3806 - outage_percentage: 0.0510 - val_loss: -0.3783 - val_Delta_DNN: 0.1697 - val_opportunistic_rate: 0.3817 - val_outage_percentage: 0.0501\n",
      "Epoch 65/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3763 - Delta_DNN: 0.1704 - opportunistic_rate: 0.3797 - outage_percentage: 0.0515 - val_loss: -0.3781 - val_Delta_DNN: 0.1705 - val_opportunistic_rate: 0.3813 - val_outage_percentage: 0.0439\n",
      "Epoch 66/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3761 - Delta_DNN: 0.1704 - opportunistic_rate: 0.3797 - outage_percentage: 0.0515 - val_loss: -0.3787 - val_Delta_DNN: 0.1702 - val_opportunistic_rate: 0.3816 - val_outage_percentage: 0.0418\n",
      "Epoch 67/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3779 - Delta_DNN: 0.1712 - opportunistic_rate: 0.3813 - outage_percentage: 0.0520 - val_loss: -0.3788 - val_Delta_DNN: 0.1725 - val_opportunistic_rate: 0.3835 - val_outage_percentage: 0.0617\n",
      "Epoch 68/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3779 - Delta_DNN: 0.1709 - opportunistic_rate: 0.3814 - outage_percentage: 0.0521 - val_loss: -0.3784 - val_Delta_DNN: 0.1693 - val_opportunistic_rate: 0.3803 - val_outage_percentage: 0.0305\n",
      "Epoch 69/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3759 - Delta_DNN: 0.1715 - opportunistic_rate: 0.3792 - outage_percentage: 0.0513 - val_loss: -0.3793 - val_Delta_DNN: 0.1708 - val_opportunistic_rate: 0.3818 - val_outage_percentage: 0.0427\n",
      "Epoch 70/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3778 - Delta_DNN: 0.1722 - opportunistic_rate: 0.3810 - outage_percentage: 0.0519 - val_loss: -0.3787 - val_Delta_DNN: 0.1731 - val_opportunistic_rate: 0.3839 - val_outage_percentage: 0.0705\n",
      "Epoch 71/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3763 - Delta_DNN: 0.1712 - opportunistic_rate: 0.3797 - outage_percentage: 0.0520 - val_loss: -0.3793 - val_Delta_DNN: 0.1712 - val_opportunistic_rate: 0.3819 - val_outage_percentage: 0.0422\n",
      "Epoch 72/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3767 - Delta_DNN: 0.1726 - opportunistic_rate: 0.3800 - outage_percentage: 0.0521 - val_loss: -0.3797 - val_Delta_DNN: 0.1740 - val_opportunistic_rate: 0.3834 - val_outage_percentage: 0.0587\n",
      "Epoch 73/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3787 - Delta_DNN: 0.1733 - opportunistic_rate: 0.3818 - outage_percentage: 0.0521 - val_loss: -0.3790 - val_Delta_DNN: 0.1741 - val_opportunistic_rate: 0.3833 - val_outage_percentage: 0.0608\n",
      "Epoch 74/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3785 - Delta_DNN: 0.1734 - opportunistic_rate: 0.3815 - outage_percentage: 0.0519 - val_loss: -0.3798 - val_Delta_DNN: 0.1730 - val_opportunistic_rate: 0.3821 - val_outage_percentage: 0.0397\n",
      "Epoch 75/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3774 - Delta_DNN: 0.1729 - opportunistic_rate: 0.3806 - outage_percentage: 0.0517 - val_loss: -0.3801 - val_Delta_DNN: 0.1747 - val_opportunistic_rate: 0.3840 - val_outage_percentage: 0.0617\n",
      "Epoch 76/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3785 - Delta_DNN: 0.1734 - opportunistic_rate: 0.3815 - outage_percentage: 0.0517 - val_loss: -0.3803 - val_Delta_DNN: 0.1742 - val_opportunistic_rate: 0.3831 - val_outage_percentage: 0.0478\n",
      "Epoch 77/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3776 - Delta_DNN: 0.1742 - opportunistic_rate: 0.3806 - outage_percentage: 0.0521 - val_loss: -0.3797 - val_Delta_DNN: 0.1711 - val_opportunistic_rate: 0.3819 - val_outage_percentage: 0.0373\n",
      "Epoch 78/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3786 - Delta_DNN: 0.1739 - opportunistic_rate: 0.3817 - outage_percentage: 0.0517 - val_loss: -0.3805 - val_Delta_DNN: 0.1752 - val_opportunistic_rate: 0.3841 - val_outage_percentage: 0.0623\n",
      "Epoch 79/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3788 - Delta_DNN: 0.1749 - opportunistic_rate: 0.3817 - outage_percentage: 0.0525 - val_loss: -0.3799 - val_Delta_DNN: 0.1720 - val_opportunistic_rate: 0.3812 - val_outage_percentage: 0.0247\n",
      "Epoch 80/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3777 - Delta_DNN: 0.1744 - opportunistic_rate: 0.3806 - outage_percentage: 0.0516 - val_loss: -0.3735 - val_Delta_DNN: 0.1821 - val_opportunistic_rate: 0.3882 - val_outage_percentage: 0.1669\n",
      "Epoch 81/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3775 - Delta_DNN: 0.1726 - opportunistic_rate: 0.3808 - outage_percentage: 0.0537 - val_loss: -0.3806 - val_Delta_DNN: 0.1752 - val_opportunistic_rate: 0.3836 - val_outage_percentage: 0.0493\n",
      "Epoch 82/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3799 - Delta_DNN: 0.1754 - opportunistic_rate: 0.3827 - outage_percentage: 0.0518 - val_loss: -0.3796 - val_Delta_DNN: 0.1707 - val_opportunistic_rate: 0.3817 - val_outage_percentage: 0.0347\n",
      "Epoch 83/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3793 - Delta_DNN: 0.1751 - opportunistic_rate: 0.3822 - outage_percentage: 0.0526 - val_loss: -0.3792 - val_Delta_DNN: 0.1693 - val_opportunistic_rate: 0.3806 - val_outage_percentage: 0.0253\n",
      "Epoch 84/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3795 - Delta_DNN: 0.1747 - opportunistic_rate: 0.3823 - outage_percentage: 0.0515 - val_loss: -0.3800 - val_Delta_DNN: 0.1775 - val_opportunistic_rate: 0.3853 - val_outage_percentage: 0.0800\n",
      "Epoch 85/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3806 - Delta_DNN: 0.1762 - opportunistic_rate: 0.3833 - outage_percentage: 0.0527 - val_loss: -0.3810 - val_Delta_DNN: 0.1763 - val_opportunistic_rate: 0.3833 - val_outage_percentage: 0.0393\n",
      "Epoch 86/500\n",
      "196/196 [==============================] - 1s 7ms/step - loss: -0.3808 - Delta_DNN: 0.1758 - opportunistic_rate: 0.3835 - outage_percentage: 0.0513 - val_loss: -0.3813 - val_Delta_DNN: 0.1774 - val_opportunistic_rate: 0.3846 - val_outage_percentage: 0.0616\n",
      "Epoch 87/500\n",
      " 77/196 [==========>...................] - ETA: 0s - loss: -0.3807 - Delta_DNN: 0.1770 - opportunistic_rate: 0.3832 - outage_percentage: 0.0527"
     ]
    }
   ],
   "source": [
    "Nbr_train = int(1E6)\n",
    "\n",
    " \n",
    "tau = 0.25\n",
    "\n",
    "VS = 0.2 # validation_split\n",
    "\n",
    "Epochs = 500 # Epochs number\n",
    "\n",
    "BS = 4096 # batch_size\n",
    "\n",
    "LD = 10**0.5\n",
    "\n",
    "LR = 10**-4\n",
    "\n",
    "metrics = [Delta_DNN, Opportunistic_Achievable_Rate(tau),outage_percentage(tau)] #, QoS_mean_DF, QoS_median_DF\n",
    "\n",
    "\n",
    "\n",
    "#Nbr_train = int(1E6)\n",
    "\n",
    "model = get_model_CF(x_train, loss_CF(LD,tau), metrics, custom_sigmoid, custom_sigmoid, LR) #lr_v\n",
    "history = model.fit(x_train, np.power(x_train,2), epochs=Epochs, batch_size=BS, validation_split = VS)\n",
    "\n",
    "model.save('WM_CF/CF.h5')\n",
    "np.save('WM_CF/CF',history.history)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578a79a",
   "metadata": {},
   "source": [
    "# Part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f79e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.load('WM_CF/CF.npy',allow_pickle='TRUE').item()\n",
    "\n",
    "fig, ax = plt.subplots(1,figsize=(15,7), sharey=True)\n",
    "ax.grid()\n",
    "ax.plot(history['loss'][:],color='black',lw=2,label='loss')\n",
    "ax.plot(history['val_loss'][:],color='blue',lw=2,label='val_loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Epochs\",fontsize=24)\n",
    "plt.ylabel(\"Loss\",fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.load('WM_CF/CF.npy',allow_pickle='TRUE').item()\n",
    "\n",
    "fig, ax = plt.subplots(1,figsize=(15,7), sharey=True)\n",
    "ax.grid()\n",
    "ax.plot(history['Delta_DNN'][:],color='black',lw=2,label='Delta')\n",
    "ax.plot(history['val_Delta_DNN'][:],color='blue',lw=2,label='val_Delta')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Epochs\",fontsize=24)\n",
    "plt.ylabel(\"Delta\",fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7787b1c2",
   "metadata": {},
   "source": [
    "# Part 3 channel noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac66a35",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_to_channels(X, primary_ID, secondary_ID, SNRs_db = [-10, -5, 0, 5, 10, 15, 20]):\n",
    "    '''\n",
    "    Parameters : \n",
    "       \n",
    "        test_set :  test set containing the H channels\n",
    "    \n",
    "        col : list of index for the specific column to add noise\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        channel gain ndarray container of noisy channels \n",
    "\n",
    "    '''   \n",
    "   \n",
    "    #static_var_X = np.sqrt(np.var(X[:, primary_ID], axis=0))\n",
    "\n",
    "    #var_X = np.var(X[:, secondary_ID], axis=0, keepdims=True)\n",
    "    var_X = np.var(X[:, secondary_ID+primary_ID], axis=0, keepdims=True)\n",
    "    \n",
    "    noisy_gains = [] # list to store all the noisy H matrices with different level of noise variance\n",
    "    \n",
    "    for SNR_db in SNRs_db:\n",
    "        \n",
    "        SNR = np.power(10,SNR_db/10)\n",
    "        noises = np.sqrt(var_X/SNR)*np.random.normal(0.0, 1.0, (X.shape[0], len(secondary_ID+primary_ID)))#len(secondary_ID)\n",
    "    \n",
    "        X_noised = X.copy()\n",
    "        #X_noised[:, secondary_ID] = X_noised[:, secondary_ID] + noises\n",
    "        X_noised[:, secondary_ID+primary_ID] = X_noised[:, secondary_ID+primary_ID] + noises\n",
    "        #X_noised[:, primary_ID] = static_var_X\n",
    "        noisy_gains.append(X_noised)\n",
    "\n",
    "    return SNRs_db, np.asarray(noisy_gains, dtype=\"float64\")\n",
    "#-----------------------------------------------------------------------------------------#\n",
    "\n",
    "def AS_A_squeeze(x):\n",
    "    return CF_V4(x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7])\n",
    "\n",
    "def generate_benchmark(H_matrix): \n",
    "    '''\n",
    "    bruteforce for H without noise\n",
    "    '''\n",
    "    with Pool() as p:\n",
    "        BF_res =  p.map(AS_A_squeeze, H_matrix)\n",
    "\n",
    "    return np.squeeze(np.asarray(BF_res, dtype=\"float64\"))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------#\n",
    "\n",
    "def AS_for_noisy_channels(BH_matrix):\n",
    "    \n",
    "    '''\n",
    "    Compute bruteforce method for channel gain ndarray composed of noisy channels\n",
    "    '''\n",
    "\n",
    "    BF_res = [] # list containing channels and bruteforce results (Alpha,Pr,Ps) for each noisy matrix (0, 10^-1.5, 10^-1....) \n",
    "    \n",
    "    \n",
    "    for i in range(BH_matrix.shape[0]) :\n",
    "        X = BH_matrix[i,:,:]\n",
    "        \n",
    "        temp_BF_res = generate_benchmark(X)\n",
    "        \n",
    "        BF_res.append(temp_BF_res)\n",
    "\n",
    "        \n",
    "    return np.asarray(BF_res, dtype=\"float64\")\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1840878c",
   "metadata": {},
   "source": [
    "## Analytic solution to noisy test set and test set  without noise ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "N2, NR, N1= 1.0, 1.0, 1.0\n",
    "P2_max, PR_max, P1 = 10.0, 10.0, 10.0\n",
    "tau = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Noisy H-MATRIX with different noise variance\n",
    "\n",
    "noise_levels, NH_MATRIX = noise_to_channels(x_test, Primary_ID, Secondary_ID)\n",
    "\n",
    "# Compute bruteforce for NH-MATRIX\n",
    "\n",
    "AS_NG_MATRIX = AS_for_noisy_channels(NH_MATRIX) \n",
    "\n",
    "# bruteforce for test_set without noise\n",
    "\n",
    "AS_G_Benchmark = generate_benchmark(x_test) \n",
    "\n",
    "#----------------------------------Save data----------------------------------------------#\n",
    "\n",
    "outfile_NH_MATRIX = 'Imperfect CSI Data/CF_DNN/NH_MATRIX_GF_CDIT'\n",
    "np.savez(outfile_NH_MATRIX, NH_MATRIX)\n",
    "\n",
    "\n",
    "outfile_BF_NG_MATRIX = 'Imperfect CSI Data/CF_DNN/BF_NG_MATRIX_GF_CDIT'\n",
    "np.savez(outfile_BF_NG_MATRIX, AS_NG_MATRIX)\n",
    "\n",
    "\n",
    "outfile_BF_G_Benchmark = 'Imperfect CSI Data/CF_DNN/BF_G_Benchmark_GF_CDIT'\n",
    "np.savez(outfile_BF_G_Benchmark, AS_G_Benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b953c82",
   "metadata": {},
   "source": [
    "## loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e871818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load ... \n",
    "# useful for DNN prediction because DNN takes H as input \n",
    "\n",
    "outfile_NH_MATRIX = 'Imperfect CSI Data/CF_DNN/NH_MATRIX_GF_CDIT'\n",
    "\n",
    "NH_MATRIX = np.load(outfile_NH_MATRIX+\".npz\")\n",
    "NH_MATRIX.files\n",
    "NH_MATRIX = NH_MATRIX['arr_0']\n",
    "\n",
    "# Noisy_H ==> NH_MATRIX\n",
    "\n",
    "#-----------------------------------------------------------------------------------------#\n",
    "# useful for Rate, outage, Delta calculation ...  because it containing (Alpha, Pr, Ps) \n",
    "\n",
    "\n",
    "outfile_BF_NG_MATRIX = 'Imperfect CSI Data/CF_DNN/BF_NG_MATRIX_GF_CDIT'\n",
    "\n",
    "dataset_test = np.load(outfile_BF_NG_MATRIX+\".npz\")\n",
    "dataset_test.files\n",
    "dataset_test = dataset_test['arr_0']\n",
    "\n",
    "#-----------------------------------------------------------------------------------------#\n",
    "# useful for Rate, outage, Delta calculation, because it containing G Matrix without noise \n",
    "\n",
    "\n",
    "outfile_BF_G_Benchmark = 'Imperfect CSI Data/CF_DNN/BF_G_Benchmark_GF_CDIT'\n",
    "\n",
    "dataset = np.load(outfile_BF_G_Benchmark+\".npz\")\n",
    "dataset.files\n",
    "dataset = dataset['arr_0']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0f839",
   "metadata": {},
   "source": [
    "## test Analytic solution with noisy channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d45c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "def opportunistic_rate_for_noisy_channels(datas, labels):\n",
    "    '''\n",
    "    Parameters: \n",
    "        dataset : test_set\n",
    "        labels : For DNN estimations\n",
    "    '''\n",
    "    H_matrix = []\n",
    "    opportunistic_rate = []\n",
    "    # create H_matrix ( without noise ) for each noise level labels\n",
    "    for i in range(labels.shape[0]):\n",
    "        H_matrix.append(datas)\n",
    "    H_matrix = np.asarray(H_matrix, dtype=\"float64\")\n",
    "        \n",
    "    # calculate rate for the same H with different labels based on noise level\n",
    "    for i,j in zip(H_matrix,labels) : \n",
    "        data , label = i[:,0:8], j[:,8:12]\n",
    "        temp_rate = C(f_obj(data[:,0], data[:,1], data[:,2], data[:,3], data[:,4], data[:,5], data[:,6], data[:,7], label[:,0], label[:,1]))\n",
    "\n",
    "        opportunistic_rate.append(temp_rate)\n",
    "        \n",
    "    return np.asarray(opportunistic_rate, dtype=\"float64\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_CSI_Imperfect_Stats(res_stats, plot_lab, x_lab, y_lab, SNRs_db = [-10, -5, 0, 5, 10, 15, 20]):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,figsize=(15,7))\n",
    "\n",
    "    ax.plot(SNRs_db, res_stats, c = 'black',label= plot_lab)\n",
    "\n",
    "    ax.grid()\n",
    "\n",
    "    ax.scatter(SNRs_db[0],res_stats[0],label=r'$SNR=-10$',s=200,marker='v',linewidths=1)\n",
    "\n",
    "    ax.scatter(SNRs_db[1],res_stats[1],label=r'$SNR=-5}$',s=200,marker='o',linewidths=1)\n",
    "\n",
    "    ax.scatter(SNRs_db[2],res_stats[2],label=r'$SNR=0$',s=200,marker='D',linewidths=1)\n",
    "\n",
    "    ax.scatter(SNRs_db[3],res_stats[3],label='$SNR=5$',s=200,marker='H',linewidths=1)\n",
    "\n",
    "    ax.scatter(SNRs_db[4],res_stats[4],label='$SNR=10$',s=200,marker='d',linewidths=1)\n",
    "\n",
    "    ax.scatter(SNRs_db[5],res_stats[5],label='$SNR=15$',s=200,marker='P',linewidths=1)\n",
    "\n",
    "    ax.scatter(SNRs_db[6],res_stats[6],label='$SNR=20$',s=200,marker='<',linewidths=1)\n",
    "\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=16 )\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    ax.legend(loc='best', fontsize=16)\n",
    "\n",
    "    plt.xlabel(x_lab, fontsize=24) #'Noise variance','Percentage'\n",
    "    plt.ylabel(y_lab, fontsize=24)\n",
    "    #plt.xscale('log')\n",
    "    \n",
    "\n",
    "    #fig.savefig('Datasetv2/Bruteforce/Dataset_for_BF/RAG_opportunistic_rate.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "def plot_ARPD(mean_ARPD, maximum_ARPD, mean_outage_ARPD, SNRs_db = [-10, -5, 0, 5, 10, 15, 20]):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,figsize=(15,7))\n",
    "\n",
    "    ax.plot(SNRs_db, maximum_ARPD, c = 'blue',label=r\"$Max$\",lw=2.5)\n",
    "    ax.plot(SNRs_db, mean_ARPD, c = 'red',label=r\"$Mean$\",lw=2.5)\n",
    "    ax.plot(SNRs_db, mean_outage_ARPD, c = 'gray',label=r\"$Mean\\; outage$\",lw=2.5)\n",
    "\n",
    "    ax.grid()\n",
    "\n",
    "    xs = np.linspace(1, 10**1.5, 20)\n",
    "\n",
    "    plt.hlines(y=25, xmin=-10, xmax=len(xs), colors='black', linestyles='--', lw=2, label=r'$\\tau = 25\\%$')\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=16 )\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "\n",
    "\n",
    "    ax.legend(loc='upper center', fontsize=16)\n",
    "\n",
    "    plt.xlabel('SNR (dB)', fontsize=24)\n",
    "    plt.ylabel('Primary network degradation (%)', fontsize=24)\n",
    "    #plt.xscale('log')\n",
    "\n",
    "    #fig.savefig('Datasetv2/Bruteforce/Dataset_for_BF/PDD.pdf', bbox_inches='tight')\n",
    "\n",
    "def primary_rate_CF(hR2, h12, h1R, hR1, h22, h2R, h21, h11, PR, PS):\n",
    "\n",
    "    R_P = C((h11**2*P1)/(hR1**2*PR+h21**2*PS+1))\n",
    "\n",
    "    return R_P\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mean_max_outage_PARD(N_P_Rate, Max_P_Rate, tau = 0.25):\n",
    "    '''\n",
    "    N_PRate : Primary rate based on bruteforce output (Alpha, Pr, Ps) where BF is computed using noisy channels \n",
    "    Max_P_Rate : Primary rate without secondary users interference\n",
    "    '''\n",
    "    res = 1-(N_P_Rate/Max_P_Rate) # Compute of Delta\n",
    "    \n",
    "    #res = np.round(res,4)\n",
    "    mean_res = np.nanmean(res) # Mean of Delta \n",
    "    max_res = np.max(res) # Max of Delta\n",
    "    #res =  # Outage\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('error')\n",
    "        try:\n",
    "            mean_outage = np.nanmean(res[res>tau])\n",
    "        except RuntimeWarning:\n",
    "            mean_outage = 0\n",
    "            \n",
    "    outage = np.nanmean(res>tau)\n",
    "    \n",
    "    return mean_res, max_res, mean_outage, outage\n",
    "\n",
    "\n",
    "def PARD_for_noisy_data(datas, labels, Pp=10.0):\n",
    "    \"\"\"\n",
    "    Calculate primary rate for Noisy H Matrix\n",
    "    \"\"\"\n",
    "    H_matrix, mean_pard, max_pard, mean_outage_pard, outage_pard = [], [], [], [], []\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        H_matrix.append(datas)\n",
    "    H_matrix = np.asarray(H_matrix, dtype=\"float64\")\n",
    "    \n",
    "    for i,j in zip(H_matrix,labels) : \n",
    "        \n",
    "        data , label = i[:,0:8], j[:,8:11]\n",
    "        \n",
    "        N_P_Rate = primary_rate_CF(data[:,0], data[:,1], data[:,2], data[:,3], data[:,4], data[:,5], data[:,6], data[:,7], label[:,0], label[:,1])\n",
    "        \n",
    "        P_Rate_max = C(data[:,1]**2*Pp) # data[:,1] tend for G_PP\n",
    "        temp_mean_pard, temp_max_pard, temp_mean_outage, outage = mean_max_outage_PARD(N_P_Rate, P_Rate_max)\n",
    "        mean_pard.append(temp_mean_pard)\n",
    "        max_pard.append(temp_max_pard)\n",
    "        mean_outage_pard.append(temp_mean_outage)\n",
    "        outage_pard.append(outage)\n",
    "    # doesn't multiply outage by 100    \n",
    "    return np.asarray(mean_pard, dtype=\"float64\")*100, np.asarray(max_pard, dtype=\"float64\")*100, np.asarray(mean_outage_pard, dtype=\"float64\")*100, np.asarray(outage_pard, dtype=\"float64\")*100\n",
    "\n",
    "\n",
    "\n",
    "def relative_avreage_gap(X, Y):\n",
    "    \n",
    "    \"\"\"relative avreage gap between the predicted debit and the obtained debit based on bruteforce\"\"\"\n",
    "\n",
    "    return (np.mean(X) - np.mean(Y))/(np.mean(Y))       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c16a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_bruteforce = opportunistic_rate_for_noisy_channels(dataset, dataset_test) \n",
    "# rate for H without noise\n",
    "db_benchmark = C(f_obj(dataset[:,0], dataset[:,1], dataset[:,2], dataset[:,3], dataset[:,4], dataset[:,5], dataset[:,6], dataset[:,7], dataset[:,8], dataset[:,9]))\n",
    "\n",
    "\n",
    "# List of relative gap between each noisy matrix and benchmark (BF results without noise)\n",
    "\n",
    "db_gap = np.array([relative_avreage_gap(db_bruteforce[0,:], db_benchmark[:]),\\\n",
    "                   relative_avreage_gap(db_bruteforce[1,:], db_benchmark[:]),\\\n",
    "                   relative_avreage_gap(db_bruteforce[2,:], db_benchmark[:]),\\\n",
    "                   relative_avreage_gap(db_bruteforce[3,:], db_benchmark[:]),\\\n",
    "                   relative_avreage_gap(db_bruteforce[4,:], db_benchmark[:]),\\\n",
    "                   relative_avreage_gap(db_bruteforce[5,:], db_benchmark[:]),\\\n",
    "                   relative_avreage_gap(db_bruteforce[6,:], db_benchmark[:])])\n",
    "\n",
    "db_gap = db_gap*100\n",
    "\n",
    "\n",
    "\n",
    "SNRs_db = [-10, -5, 0, 5, 10, 15, 20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f089c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CSI_Imperfect_Stats(db_gap, 'Relative Average Gap', 'SNR (dB)','Percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b371c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_ARPD, Max_ARPD, Mean_Outage_ARPD, Outage_ARPD = PARD_for_noisy_data(dataset,dataset_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ARPD(Mean_ARPD, Max_ARPD, Mean_Outage_ARPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_CSI_Imperfect_Stats(Outage_ARPD, 'Outage', 'SNR (dB)','Outage(%)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3f6559",
   "metadata": {},
   "source": [
    "## Deep Neural Network (trained without noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3403f4",
   "metadata": {},
   "source": [
    "## test DNN with noisy channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752833f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_predictions(NH_MATRIX, mw_path, Lambda=10**0.5, tau = 0.25):\n",
    "    '''\n",
    "    NH_MATRIX : Noisy H matrix \n",
    "    '''\n",
    "    final_predictions = []\n",
    "   \n",
    "\n",
    "    model = tf.keras.models.load_model(mw_path, custom_objects={'CF_loss':loss_CF(Lambda,tau),\\\n",
    "                                                                'Delta_DNN':\\\n",
    "                                                                Delta_DNN,\\\n",
    "                                                                'outage_percentage':\\\n",
    "                                                                outage_percentage,\\\n",
    "                                                                                           \n",
    "                                                                'custom_sigmoid':custom_sigmoid,\n",
    "                                                                \"opportunistic_rate\":\\\n",
    "                                                                Opportunistic_Achievable_Rate(tau) })\n",
    "    \n",
    "    \n",
    "    for i in NH_MATRIX : \n",
    "        data = i[:,0:8]\n",
    "        temp_predictions = model.predict(data)\n",
    "        final_predictions.append(temp_predictions)\n",
    "        \n",
    "    return np.asarray(final_predictions, dtype=\"float64\")\n",
    "\n",
    "def opportunistic_rate_for_noisy_channels_DNN(datas, labels):\n",
    "    '''\n",
    "    dataset : H matrix without noise\n",
    "    labels : predicted parameters (Alpha, Pr, Ps)\n",
    "    #  Warning : rate is computed using y_hat not sqrt(y_hat) like opportunistic_rate_for_noisy_channels function\n",
    "\n",
    "    '''\n",
    "    final_rate = []\n",
    "    H_matrix = []\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        H_matrix.append(datas)\n",
    "    H_matrix = np.asarray(H_matrix, dtype=\"float64\")\n",
    "    \n",
    "    for i,j in zip(H_matrix, labels): \n",
    "        data , y_hat = i[:,0:8], j\n",
    "        temp_rate = C(f_obj(data[:,0], data[:,1], data[:,2], data[:,3], data[:,4], data[:,5], data[:,6], data[:,7], y_hat[:,0]**2, y_hat[:,1]**2))\n",
    "        final_rate.append(temp_rate)\n",
    "    return np.asarray(final_rate, dtype=\"float64\")\n",
    "\n",
    "def primary_degradation_for_noisy_data_DNN(datas, labels):\n",
    "    final_pdd = []\n",
    "    H_matrix = []\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        H_matrix.append(datas)\n",
    "    H_matrix = np.asarray(H_matrix, dtype=\"float64\")\n",
    "    \n",
    "    for i,j in zip(H_matrix, labels): \n",
    "        data, y_hat = i[:,0:8], j\n",
    "        temp_pdd = primary_rate_CF(data[:,0], data[:,1], data[:,2], data[:,3], data[:,4], data[:,5], data[:,6], data[:,7], y_hat[:,0]**2, y_hat[:,1]**2)\n",
    "        final_pdd.append(temp_pdd)\n",
    "        \n",
    "    return np.asarray(final_pdd, dtype=\"float64\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4df242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"weights_model/gaussian_fading_model/anne_model.h5\"\n",
    "Model_Weight_Path = \"WM_CF/CF.h5\"\n",
    "\n",
    "DNN_pred = DNN_predictions(NH_MATRIX, Model_Weight_Path)\n",
    "db_DNN = opportunistic_rate_for_noisy_channels_DNN(dataset, DNN_pred)\n",
    "db_benchmark = C(f_obj(dataset[:,0], dataset[:,1], dataset[:,2], dataset[:,3], dataset[:,4], dataset[:,5], dataset[:,6], dataset[:,7], dataset[:,8], dataset[:,9]))\n",
    "\n",
    "\n",
    "db_gap_DNN = np.array([relative_avreage_gap(db_DNN[0,:], db_benchmark[:]),\\\n",
    "                       relative_avreage_gap(db_DNN[1,:], db_benchmark[:]),\\\n",
    "                       relative_avreage_gap(db_DNN[2,:], db_benchmark[:]),\\\n",
    "                       relative_avreage_gap(db_DNN[3,:], db_benchmark[:]),\\\n",
    "                       relative_avreage_gap(db_DNN[4,:], db_benchmark[:]),\\\n",
    "                       relative_avreage_gap(db_DNN[5,:], db_benchmark[:]),\\\n",
    "                       relative_avreage_gap(db_DNN[6,:], db_benchmark[:])])\n",
    "#,\\\n",
    "#                       relative_avreage_gap(db_DNN[7,:,8], db_benchmark[:,8])\n",
    "\n",
    "\n",
    "#ax.plot(variance, db_gap, c = 'blue',label='BF')\n",
    "\n",
    "db_gap_DNN = db_gap_DNN*100\n",
    "\n",
    "plot_CSI_Imperfect_Stats(db_gap_DNN, 'Relative Average Gap', 'SNR (dB)','Percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PARD_for_noisy_data_DNN(datas, labels, Pp = 10.0):\n",
    "    \"\"\"\n",
    "    Calculate primary rate for Noisy H Matrix\n",
    "    \"\"\"\n",
    "    H_matrix, mean_pard, max_pard, mean_outage_pard, outage_pard = [], [], [], [], []\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        H_matrix.append(datas)\n",
    "    H_matrix = np.asarray(H_matrix, dtype=\"float64\")\n",
    "    \n",
    "    for i,j in zip(H_matrix,labels) : \n",
    "        \n",
    "        data , label = i[:,0:8], j # j not j[:,8:12]\n",
    "        # label for primary_rate not sqrt label\n",
    "        N_P_Rate = primary_rate_CF(data[:,0], data[:,1], data[:,2], data[:,3], data[:,4], data[:,5], data[:,6], data[:,7], label[:,0]**2, label[:,1]**2)\n",
    "\n",
    "        P_Rate_max = C(data[:,1]**2*Pp) # data[:,1] tend for G_PP\n",
    "\n",
    "        temp_mean_pard, temp_max_pard, temp_mean_outage, outage = mean_max_outage_PARD(N_P_Rate, P_Rate_max)\n",
    "        mean_pard.append(temp_mean_pard)\n",
    "        max_pard.append(temp_max_pard)\n",
    "        mean_outage_pard.append(temp_mean_outage)\n",
    "        outage_pard.append(outage)\n",
    "    # doesn't multiply outage by 100    \n",
    "    return np.asarray(mean_pard, dtype=\"float64\")*100, np.asarray(max_pard, dtype=\"float64\")*100, np.asarray(mean_outage_pard, dtype=\"float64\")*100, np.asarray(outage_pard, dtype=\"float64\")*100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Mean_ARPD, Max_ARPD, Mean_Outage_ARPD, Outage_ARPD = PARD_for_noisy_data_DNN(dataset, DNN_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_Outage_ARPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8260b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ARPD(Mean_ARPD, Max_ARPD, Mean_Outage_ARPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3233baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_CSI_Imperfect_Stats(Outage_ARPD, 'Outage', 'SNR (dB)','Outage(%)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec520fdc",
   "metadata": {},
   "source": [
    "## Training DNN with noisy channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ccae12",
   "metadata": {},
   "source": [
    "## Noise to train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313672c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get P and S channels index form dataset (h_R1, h_11, h_2R, h_1R, h_22, h_R2, h_21, h_12)\n",
    "\n",
    "Primary_ID = [0, 1, 6] \n",
    "Secondary_ID = [3, 7]\n",
    "\n",
    "\n",
    "noise_levels, X_train_noised = noise_to_channels(x_train, Primary_ID, Secondary_ID)\n",
    "X_train_noised_path = 'Imperfect CSI Data/CF_DNN/X_train_noised_GF_CDIT'\n",
    "np.savez(X_train_noised_path, X_train_noised)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_noised_GF = 'Dataset_VF/X_train_noised_anne'\n",
    "X_train_noised_path = 'Imperfect CSI Data/CF_DNN/X_train_noised_GF_CDIT'\n",
    "\n",
    "X_train_noised = np.load(X_train_noised_path+\".npz\")\n",
    "X_train_noised.files\n",
    "X_train_noised = X_train_noised['arr_0']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_for_noisy_channels(X, X_noised_train, hyperparameters, SNRs_db = [-10, -5, 0, 5, 10, 15, 20]):\n",
    "    #X_noised_train, X_noised_val,hyperparameters, noise_levels = [0, 10**-1.5, 10**-1, 10**-0.5, 1, 10**0.5, 10**1, 10**1.5]):\n",
    "    # SNRs_db = [-10, -5, 0, 5, 10, 15, 20]\n",
    "    final_DNN_results = []\n",
    "    \n",
    "    ind = 0 \n",
    "    \n",
    "    for x_noisy in X_noised_train : \n",
    "\n",
    "        model = get_model_CF(x_noisy, loss_CF(hyperparameters[\"lambda\"],\\\n",
    "                                                 hyperparameters[\"tau\"]),\\\n",
    "                                                 hyperparameters[\"metrics\"],\\\n",
    "                                                 custom_sigmoid,\\\n",
    "                                                 custom_sigmoid,\\\n",
    "                                                 hyperparameters[\"learning_rate\"])\n",
    "\n",
    "        history = model.fit(x_noisy, np.power(X, 2), epochs=hyperparameters[\"epochs\"], batch_size=hyperparameters[\"batch_size\"], validation_split=hyperparameters[\"validation_split\"])#validationhyperparameters[\"batch_size\"]_split = VS\n",
    "        \n",
    "        model.save('Weights_Model/CSI imperfection/CDIT/v2_'+str(SNRs_db[ind])+'.h5')\n",
    "        np.save('Weights_Model/CSI imperfection/CDIT/v2_'+str(SNRs_db[ind]),history.history)\n",
    "        \n",
    "        ind+=1\n",
    "#-----------------------------------------------------------------------------------------#        \n",
    "Nbr_train = int(1E6)\n",
    "\n",
    " \n",
    "tau = 0.25\n",
    "\n",
    "VS = 0.2 # validation_split\n",
    "\n",
    "Epochs = 500 # Epochs number\n",
    "\n",
    "BS = 4096 # batch_size\n",
    "\n",
    "LD = 10**0.5\n",
    "\n",
    "LR = 10**-4\n",
    "\n",
    "\n",
    "metrics = [Delta_DNN, Opportunistic_Achievable_Rate(tau),outage_percentage(tau)] #, QoS_mean_DF, QoS_median_DF\n",
    "\n",
    "\n",
    "\n",
    "#Nbr_train = int(1E6)\n",
    "\n",
    "hyperparameters = {'lambda' : LD, 'tau': tau, 'metrics': metrics, 'learning_rate': LR, 'epochs':Epochs, 'batch_size': BS, 'validation_split': VS}\n",
    "#hyperparameters = [LD, tau, metrics, LR, Epochs, BS, VS]\n",
    "DNN_for_noisy_channels(x_train[:Nbr_train,:], X_train_noised[:,:Nbr_train,:], hyperparameters)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26903de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noised_DNN_pred_V2(X, mw_path, SNRs_db = [-10, -5, 0, 5, 10, 15, 20]):\n",
    "    # SNRs_db = [-10, -5, 0, 5, 10, 15, 20]\n",
    "    final_predictions = []\n",
    "    tau = 0.25\n",
    "    for snr_item in SNRs_db : \n",
    "        for noisy_X in X:\n",
    "            data = noisy_X[:,0:8]\n",
    "            model = tf.keras.models.load_model(mw_path+str(snr_item)+'.h5', custom_objects={'CF_loss':loss_CF(Lambda,tau),\\\n",
    "                                                                'Delta_DNN':Delta_DNN,       \n",
    "                                                                 'custom_sigmoid':custom_sigmoid,\n",
    "                                                                 \"opportunistic_rate\":Opportunistic_Achievable_Rate(tau),  \n",
    "                                                                 'outage_percentage':outage_percentage    })\n",
    "            temp_predictions = model.predict(data)\n",
    "            final_predictions.append(temp_predictions)\n",
    "    \n",
    "    return np.asarray(final_predictions, dtype=\"float64\")\n",
    "\n",
    "\n",
    "\n",
    "DNN_pred = noised_DNN_pred_V2(NH_MATRIX, 'Weights_Model/CSI imperfection/CDIT/v2_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fa5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_gap_Dnn(acheivable_rate, acheivable_rate_bruteforce, mul):\n",
    "    \n",
    "    db_gap_DNN = np.array([relative_avreage_gap(acheivable_rate[mul,:],\\\n",
    "                                                acheivable_rate_bruteforce[:])])\n",
    "    return db_gap_DNN*100\n",
    "\n",
    "def opportunistic_rate_for_noisy_channels_DNN(datas, labels):\n",
    "    '''\n",
    "    dataset : H matrix without noise\n",
    "    labels : predicted parameters (Alpha, Pr, Ps)\n",
    "    #  Warning : rate is computed using y_hat not sqrt(y_hat) like opportunistic_rate_for_noisy_channels function\n",
    "\n",
    "    '''\n",
    "    final_rate = []\n",
    "    H_matrix = []\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        H_matrix.append(datas)\n",
    "    H_matrix = np.asarray(H_matrix, dtype=\"float64\")\n",
    "    \n",
    "    for i,j in zip(H_matrix, labels): \n",
    "        data , y_hat = i[:,0:8], j\n",
    "\n",
    "        temp_rate = C(f_obj(data[:,0], data[:,1], data[:,2], data[:,3], data[:,4], data[:,5], data[:,6], data[:,7], y_hat[:,0]**2, y_hat[:,1]**2))\n",
    "\n",
    "\n",
    "        final_rate.append(temp_rate)\n",
    "    return np.asarray(final_rate, dtype=\"float64\")\n",
    "\n",
    "\n",
    "def PARD_for_noisy_data_DNN(datas, labels, Pp = 10.0):\n",
    "    \"\"\"\n",
    "    Calculate primary rate for Noisy H Matrix\n",
    "    \"\"\"\n",
    "    H_matrix, mean_pard, max_pard, mean_outage_pard, outage_pard = [], [], [], [], []\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        H_matrix.append(datas)\n",
    "    H_matrix = np.asarray(H_matrix, dtype=\"float64\")\n",
    "    \n",
    "    for i,j in zip(H_matrix,labels) : \n",
    "        \n",
    "        data , label = i[:,0:8], j # j not j[:,8:12]\n",
    "        # label for primary_rate not sqrt label\n",
    "        N_P_Rate = primary_rate_CF(data[:,0], data[:,1], data[:,2], data[:,3], data[:,4], data[:,5], data[:,6], data[:,7], label[:,0]**2, label[:,1]**2)\n",
    "\n",
    "\n",
    "        P_Rate_max = C(data[:,1]**2*Pp) # data[:,1] tend for G_PP\n",
    "\n",
    "        temp_mean_pard, temp_max_pard, temp_mean_outage, outage = mean_max_outage_PARD(N_P_Rate, P_Rate_max)\n",
    "        mean_pard.append(temp_mean_pard)\n",
    "        max_pard.append(temp_max_pard)\n",
    "        mean_outage_pard.append(temp_mean_outage)\n",
    "        outage_pard.append(outage)\n",
    "    # doesn't multiply outage by 100    \n",
    "    return np.asarray(mean_pard, dtype=\"float64\")*100, np.asarray(max_pard, dtype=\"float64\")*100, np.asarray(mean_outage_pard, dtype=\"float64\")*100, np.asarray(outage_pard, dtype=\"float64\")*100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f32a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_DNN = opportunistic_rate_for_noisy_channels_DNN(dataset, DNN_pred)\n",
    "db_benchmark = C(f_obj(dataset[:,0], dataset[:,1], dataset[:,2], dataset[:,3], dataset[:,4], dataset[:,5], dataset[:,6], dataset[:,7], dataset[:,8], dataset[:,9]))\n",
    "\n",
    "\n",
    "Mean_ARPD_DNN, Max_ARPD_DNN, Mean_Outage_ARPD_DNN, Outage_ARPD_DNN = PARD_for_noisy_data_DNN(dataset, DNN_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faebed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dnn_results(X, bruteforce_rate, SNR_ID= [0, 8, 16, 24, 32, 40, 48]): \n",
    "    \n",
    "    rate_gap, max_pard, mean_pard, mean_outage_pard, outage_pard  = [], [], [], [], []\n",
    "    for ID in SNR_ID : \n",
    "        rate_gap.append(db_gap_Dnn(X[\"opportunistic_rate\"], bruteforce_rate, [ID])) #X[0]\n",
    "        mean_pard.append(X[\"mean_ARPD\"][ID])\n",
    "        max_pard.append(X[\"max_ARPD\"][ID])\n",
    "        mean_outage_pard.append(X[\"mean_outage\"][ID])\n",
    "        outage_pard.append(X[\"outage_ARPD\"][ID])\n",
    "    return rate_gap, mean_pard, max_pard, mean_outage_pard, outage_pard\n",
    "\n",
    "X = {\"opportunistic_rate\":db_DNN, \"mean_ARPD\":Mean_ARPD_DNN,\"max_ARPD\":Max_ARPD_DNN,\\\n",
    "    \"mean_outage\":Mean_Outage_ARPD_DNN, \"outage_ARPD\":Outage_ARPD_DNN}\n",
    "\n",
    "rate_gap_dnn, mean_pard_dnn, max_pard_dnn, mean_outage_pard_dnn, outage_pard_dnn = get_dnn_results(X, db_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023a5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216269ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNRs_db = [-10,-5,0,5,10,15,20]\n",
    "fig, ax = plt.subplots(1, 1,figsize=(15,7), sharey=True)\n",
    "\n",
    "ax.plot(SNRs_db, rate_gap_dnn, c = 'blue',lw=2.5,label=\"DNN\")\n",
    "plt.plot([-10,-5,0,5,10,15,20], db_gap,color=\"red\",linestyle='dashed',label=\"bruteforce\")# bruteforc\n",
    "ax.grid()\n",
    "plt.xlabel(\"SNR (dB)\",fontsize=24)\n",
    "plt.ylabel(\"Percentage\",fontsize=24)\n",
    "plt.title(\"Relative Average Gap (Opportunistic Acheivable Rate)\", fontsize=16)\n",
    "plt.tick_params(axis='x', labelsize=16 )\n",
    "plt.tick_params(axis='y', labelsize=16)\n",
    "plt.legend(loc='best',fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ARPD_Subplots(mean_ARPD, maximum_ARPD, mean_outage_ARPD, SNRs_db = [-10, -5, 0, 5, 10, 15, 20]):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2,figsize=(15,7), sharey=True)\n",
    "\n",
    "    ax[0].plot(SNRs_db, maximum_ARPD[0], c = 'blue',label=r\"$Max$\",lw=2.5)\n",
    "    ax[0].plot(SNRs_db, mean_ARPD[0], c = 'red',label=r\"$Mean$\",lw=2.5)\n",
    "    ax[0].plot(SNRs_db, mean_outage_ARPD[0], c = 'gray',label=r\"$Mean\\; outage$\",lw=2.5)\n",
    "    \n",
    "    ax[1].plot(SNRs_db, maximum_ARPD[1], c = 'blue',label=r\"$Max$\",lw=2.5)\n",
    "    ax[1].plot(SNRs_db, mean_ARPD[1], c = 'red',label=r\"$Mean$\",lw=2.5)\n",
    "    ax[1].plot(SNRs_db, mean_outage_ARPD[1], c = 'gray',label=r\"$Mean\\; outage$\",lw=2.5)\n",
    "\n",
    "\n",
    "    ax[0].grid()\n",
    "    ax[1].grid()\n",
    "\n",
    "\n",
    "    xs = np.linspace(1, 10**1.5, 20)\n",
    "\n",
    "    ax[0].hlines(y=25, xmin=-10, xmax=len(xs), colors='black', linestyles='--', lw=2, label=r'$\\tau = 25\\%$')\n",
    "    ax[1].hlines(y=25, xmin=-10, xmax=len(xs), colors='black', linestyles='--', lw=2, label=r'$\\tau = 25\\%$')\n",
    "\n",
    "    \n",
    "\n",
    "    ax[0].tick_params(axis='x', labelsize=16 )\n",
    "    ax[0].tick_params(axis='y', labelsize=16)\n",
    "    ax[1].tick_params(axis='x', labelsize=16 )\n",
    "    ax[1].tick_params(axis='y', labelsize=16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ax[0].legend(loc='upper center', fontsize=16)\n",
    "    ax[1].legend(loc='upper center', fontsize=16)\n",
    "\n",
    "    \n",
    "    ax[0].set_ylabel('Primary network degradation (%)', fontsize=24)\n",
    "    ax[0].set_xlabel('SNR (dB)', fontsize=24)\n",
    "    ax[1].set_xlabel('SNR (dB)', fontsize=24)\n",
    "    #plt.xlabel('SNR (dB)', fontsize=24)\n",
    "    #plt.ylabel('Primary network degradation (%)', fontsize=24)\n",
    "    #plt.xscale('log')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eaa07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Z = [mean_pard_dnn, Mean_ARPD], [max_pard_dnn, Max_ARPD], [mean_outage_pard_dnn, Mean_Outage_ARPD]\n",
    "\n",
    "plot_ARPD_Subplots(X,Y,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991576ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1,figsize=(15,7), sharey=True)\n",
    "\n",
    "ax.plot(SNRs_db, outage_pard_dnn, c = 'blue',label=r\"$$\",lw=2.5)\n",
    "plt.plot([-10,-5,0,5,10,15,20], Outage_ARPD,color=\"red\",linestyle='dashed')# bruteforc\n",
    "ax.grid()\n",
    "plt.xlabel(\"SNR (dB)\",fontsize=24)\n",
    "plt.ylabel(\"Outage(%)\",fontsize=24)\n",
    "plt.tick_params(axis='x', labelsize=16 )\n",
    "plt.tick_params(axis='y', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef987d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
